{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk through of useful Tensor Operations\n",
    "1. Tensor Reshaping (Mostly used)\n",
    "2. Initialization of a Tensor\n",
    "3. Tensor Mathematical Operations and Comparison\n",
    "4. Tensor Indexing\n",
    "\n",
    "\n",
    "But also other things such as setting the device (GPU/CPU) and converting\n",
    "between different types (int, float etc) and how to convert a tensor to an\n",
    "numpy array and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Cuda to run on GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.tensor(\n",
    "    [[1, 2, 3], [4, 5, 6]], dtype=torch.float32, device=device, requires_grad=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires_grad就可以自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : \n",
      " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
      "x_3x3 :\n",
      " tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "x_3x3 :\n",
      " tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(9)\n",
    "# Let's say we want to reshape it to be 3x3\n",
    "\n",
    "print(\"x : \\n\",x)\n",
    "\n",
    "x_3x3 = x.view(3, 3)\n",
    "print(\"x_3x3 :\\n\",x_3x3)\n",
    "\n",
    "x_3x3 = x.reshape(3, 3)\n",
    "print(\"x_3x3 :\\n\",x_3x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([0, 3, 6, 1, 4, 7, 2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# We can also do (view and reshape are very similar)\n",
    "# and the differences are in simple terms (I'm no expert at this),\n",
    "# is that view acts on contiguous tensors meaning if the\n",
    "# tensor is stored contiguously in memory or not, whereas\n",
    "# for reshape it doesn't matter because it will copy the\n",
    "# tensor to make it contiguously stored, which might come\n",
    "# with some performance loss.\n",
    "# If we for example do:\n",
    "y = x_3x3.t()\n",
    "print(\n",
    "    y.is_contiguous()\n",
    ")  # This will return False and if we try to use view now, it won't work!\n",
    "# y.view(9) would cause an error, reshape however won't\n",
    "\n",
    "# This is because in memory it was stored [0, 1, 2, ... 8], whereas now it's [0, 3, 6, 1, 4, 7, 2, 5, 8]\n",
    "# The jump is no longer 1 in memory for one element jump (matrices are stored as a contiguous block, and\n",
    "# using pointers to construct these matrices). This is a bit complicated and I need to explore this more\n",
    "# as well, at least you know it's a problem to be cautious of! A solution is to do the following\n",
    "print(y.contiguous().view(9))  # Calling .contiguous() before view and it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add two tensors along some axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 4])\n",
      "torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.arange(1, 25)\n",
    "x1 = x1.reshape(2,3,4)\n",
    "\n",
    "x2 = x1 * 2\n",
    "print(torch.cat((x1, x2), dim=0).shape)  # Shape: 4x5\n",
    "print(torch.cat((x1, x2), dim=1).shape)  # Shape 2x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "        19, 20, 21, 22, 23, 24])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's say we want to unroll x1 into one long vector with 10 elements, we can do:\n",
    "z = x1.view(-1)  # And -1 will unroll everything\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we instead have an additional dimension and we wish to keep those as is we can do:\n",
    "batch = 64\n",
    "x = torch.rand((batch, 2, 5))\n",
    "z = x.view(\n",
    "    batch, -1\n",
    ")  # And z.shape would be 64x10, this is very useful stuff and is used all the time\n",
    "\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change a tensor's dimension for A\\*B\\*C TO A\\*C\\*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Let's say we want to switch x axis so that instead of 64x2x5 we have 64x5x2\n",
    "# I.e we want dimension 0 to stay, dimension 1 to become dimension 2, dimension 2 to become dimension 1\n",
    "# Basically you tell permute where you want the new dimensions to be, torch.transpose is a special case\n",
    "# of permute (why?)\n",
    "print(x.shape)\n",
    "z = x.permute(0, 2, 1)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split a tensor to many tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2, 2])\n",
      "torch.Size([64, 2, 2])\n",
      "torch.Size([64, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# Splits x last dimension into chunks of 2 (since 5 is not integer div by 2) the last dimension\n",
    "# will be smaller, so it will split it into two tensors: 64x2x3 and 64x2x2\n",
    "z = torch.chunk(x, chunks=3, dim=2)\n",
    "print(z[0].shape)\n",
    "print(z[1].shape)\n",
    "print(z[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add a trivial dimention 10 -> 10\\*1 or 1\\*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(\n",
    "    10\n",
    ")  # Shape is [10], let's say we want to add an additional so we have 1x10\n",
    "print(x.unsqueeze(0).shape)  # 1x10\n",
    "print(x.unsqueeze(1).shape)  # 10x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove those trivial dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape :  torch.Size([1, 1, 10])\n",
      "z.shape :  torch.Size([1, 10])\n",
      "z.shape :  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Let's say we have x which is 1x1x10 and we want to remove a dim so we have 1x10\n",
    "x = torch.arange(10).unsqueeze(0).unsqueeze(1)\n",
    "print(\"x.shape : \",x.shape) \n",
    "z = x.squeeze(0)\n",
    "print(\"z.shape : \",z.shape)\n",
    "z = x.squeeze(0).squeeze(0)\n",
    "print(\"z.shape : \",z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initializing tensors\n",
    "# set initial values 0,1 random so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(size=(3, 3))  # Tensor of shape 3x3 with uninitialized data\n",
    "x = torch.zeros((3, 3))  # Tensor of shape 3x3 with values of 0\n",
    "x = torch.rand(\n",
    "    (3, 3)\n",
    ")  # Tensor of shape 3x3 with values from uniform distribution in interval [0,1)\n",
    "x = torch.ones((3, 3))  # Tensor of shape 3x3 with values of 1\n",
    "x = torch.eye(5, 5)  # Returns Identity Matrix I, (I <-> Eye), matrix of shape 2x3\n",
    "x = torch.arange(\n",
    "    start=0, end=5, step=1\n",
    ")  # Tensor [0, 1, 2, 3, 4], note, can also do: torch.arange(11)\n",
    "x = torch.linspace(start=0.1, end=1, steps=10)  # x = [0.1, 0.2, ..., 1]\n",
    "x = torch.empty(size=(1, 5)).normal_(\n",
    "    mean=0, std=1\n",
    ")  # Normally distributed with mean=0, std=1\n",
    "x = torch.empty(size=(1, 5)).uniform_(\n",
    "    0, 1\n",
    ")  # Values from a uniform distribution low=0, high=1\n",
    "x = torch.diag(torch.ones(3))  # Diagonal matrix of shape 3x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to different data types (int64, float, double) mostly used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted int16\n",
      " {tensor.short()} 1 if nonzero\n",
      "Converted int16\n",
      " tensor([0, 1, 2, 3], dtype=torch.int16) \n",
      "\n",
      "Converted float16\n",
      " tensor([0., 1., 2., 3.], dtype=torch.float16) \n",
      "\n",
      "Converted float64\n",
      " tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(4)  # [0, 1, 2, 3] Initialized as int64 by default\n",
    "print(\"Converted int16\\n {tensor.short()} 1 if nonzero\")\n",
    "    \n",
    "print(f\"Converted int16\\n {tensor.short()} \\n\")  # Converted to int16\n",
    "\n",
    "print(f\"Converted float16\\n {tensor.half()} \\n\")  # Converted to float16\n",
    "\n",
    "print(f\"Converted float64\\n {tensor.double()}\")  # Converted to float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted float32\n",
      " tensor([0., 1., 2., 3.])\n",
      "Converted int64\n",
      " tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Converted float32\\n {tensor.float()}\"\n",
    ")  # Converted to float32 (This one is very important, used super often)\n",
    "\n",
    "print(\n",
    "    f\"Converted int64\\n {tensor.long()}\"\n",
    ")  # Converted to int64 (This one is very important, used super often)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# switch between numpy ad tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.from_numpy(np_array) ## 只能转换之后再改类型\n",
    "tensor = tensor.float()\n",
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = (\n",
    "    tensor.numpy()\n",
    ")\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([9, 8, 7])\n",
    "\n",
    "# -- Addition --\n",
    "z = x + y\n",
    "\n",
    "# -- Subtraction --\n",
    "z = x - y  # We can do similarly as the preferred way of addition\n",
    "\n",
    "# -- Division (A bit clunky) --\n",
    "z = torch.true_divide(x, y)  # Will do element wise division if of equal shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.add_(B) will change A ， A.add(B) will not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.zeros(3)\n",
    "t.add_(x)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.zeros(3)\n",
    "t.add(x)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.pow(2)  # z = [1, 4, 9]\n",
    "z = x ** 2  # z = [1, 4, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Simple Comparison --\n",
    "z = x > 0  # Returns [True, True, True]\n",
    "z = x < 0  # Returns [False, False, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication A.mm(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.rand((2, 5))\n",
    "x2 = torch.rand((5, 3))\n",
    "x3 = torch.mm(x1, x2)  # Matrix multiplication of x1 and x2, out shape: 2x3\n",
    "x3 = x1.mm(x2)  # Similar as line above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Exponentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7060, 4.8615, 1.9560, 3.4318, 2.8762],\n",
      "        [3.9800, 7.1649, 2.7901, 4.9068, 4.1118],\n",
      "        [2.8601, 5.1823, 2.0280, 3.6318, 2.9458],\n",
      "        [1.8719, 3.4413, 1.3883, 2.4582, 2.0266],\n",
      "        [2.7577, 5.0379, 2.0146, 3.5526, 2.9943]])\n"
     ]
    }
   ],
   "source": [
    "matrix_exp = torch.rand(5, 5)\n",
    "print(\n",
    "    matrix_exp.matrix_power(3)\n",
    ")  # is same as matrix_exp (mm) matrix_exp (mm) matrix_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Element wise Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x * y  # z = [9, 16, 21] = [1*9, 2*8, 3*7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot product only for 1-D tensor, for matrix have to reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(46)\n"
     ]
    }
   ],
   "source": [
    "z = torch.dot(x, y)  # Dot product, in this case z = 1*9 + 2*8 + 3*7\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 32\n",
    "n = 10\n",
    "m = 20\n",
    "p = 30\n",
    "tensor1 = torch.rand((batch, n, m))\n",
    "tensor2 = torch.rand((batch, m, p))\n",
    "out_bmm = torch.bmm(tensor1, tensor2)  # Will be shape: (b x n x p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other useful tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_x = torch.sum(\n",
    "    x, dim=0\n",
    ")  # Sum of x across dim=0 (which is the only dim in our case), sum_x = 6\n",
    "\n",
    "values, indices = torch.max(x, dim=0)  # Can also do x.max(dim=0)\n",
    "\n",
    "values, indices = torch.min(x, dim=0)  # Can also do x.min(dim=0)\n",
    "\n",
    "abs_x = torch.abs(x)  # Returns x where abs function has been applied to every element\n",
    "\n",
    "z = torch.argmax(x, dim=0)  # Gets index of the maximum value\n",
    "\n",
    "z = torch.argmin(x, dim=0)  # Gets index of the minimum value\n",
    "\n",
    "mean_x = torch.mean(x.float(), dim=0)  # mean requires x to be float\n",
    "\n",
    "z = torch.eq(x, y)  # Element wise comparison, in this case z = [False, False, False]\n",
    "\n",
    "z = torch.clamp(x, min=0)\n",
    "# All values < 0 set to 0 and values > 0 unchanged (this is exactly ReLU function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sort the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y :  tensor([9, 8, 7])\n",
      "sorted_y :  tensor([7, 8, 9])\n",
      "indices :  tensor([2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "sorted_y, indices = torch.sort(y, dim=0, descending=False)\n",
    "print('y : ',y)\n",
    "print('sorted_y : ', sorted_y )\n",
    "print(\"indices : \" , indices )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False,  True,  True,  True])\n",
      "tensor(True)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# If you want to values over max_val to be clamped, do torch.clamp(x, min=min_val, max=max_val)\n",
    "x = torch.tensor([1, 0, 1, 1, 1], dtype=torch.bool)  # True/False values\n",
    "print(x)\n",
    "z = torch.any(x)  # will return True, can also do x.any() instead of torch.any(x)\n",
    "print(z)\n",
    "z = torch.all(\n",
    "    x\n",
    ")  # will return False (since not all are True), can also do x.all() instead of torch.all()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Indexing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "features = 25\n",
    "x = torch.rand((batch_size, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "# Get first examples features\n",
    "print(x[0].shape)  # shape [25], this is same as doing x[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Get the first feature for all examples\n",
    "print(x[:, 0].shape)  # shape [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# For example: Want to access third example in the batch and the first ten features\n",
    "print(x[2, 0:10].shape)  # shape: [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example we can use this to, assign certain elements\n",
    "x[0, 0] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index with a list or lists, same in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "indices = [2, 5, 8]\n",
    "print(x[indices])  # x[indices] = [2, 5, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5067, 0.7953, 0.5864, 0.6685, 0.8093],\n",
      "        [0.1426, 0.2775, 0.5886, 0.9847, 0.7986],\n",
      "        [0.7934, 0.0702, 0.8076, 0.7612, 0.5713]])\n",
      "tensor([0.7986, 0.5067])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((3, 5))\n",
    "rows = torch.tensor([1, 0])\n",
    "cols = torch.tensor([4, 0])\n",
    "print(x)\n",
    "print(x[rows, cols])  # Gets second row fifth column and first row first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select sublist according to value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 1, 9])\n",
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([ 0,  2,  4,  6,  8, 10,  6,  7,  8,  9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "print(x)\n",
    "print(x[(x < 2) | (x > 8)])  # will be [0, 1, 9]\n",
    "print(x[x.remainder(2) == 0])  # depend on value not position\n",
    "print(\n",
    "    torch.where(x > 5, x, x * 2)\n",
    ")  # gives [0, 2, 4, 6, 8, 10, 6, 7, 8, 9], all values x > 5 yield x, else x*2\n",
    "x = torch.tensor([0, 0, 1, 2, 2, 3, 4]).unique()  # x = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension 1\n",
      "The number of elements 15\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'dimension' , x.ndimension()\n",
    ")  # The number of dimensions, in this case 1. if x.shape is 5x5x5 ndim would be 3\n",
    "\n",
    "x = torch.rand((3, 5))\n",
    "print(\n",
    "    'The number of elements' , x.numel()\n",
    ")  # The number of elements in x (in this case it's trivial because it's just a vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
